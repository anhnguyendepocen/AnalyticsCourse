---
title: "Week 8 Assignment"
author: "Put your name here"
date: "`r Sys.Date()`"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Train a New Model

Your objective is to train two machine learning models - a logistic regression model and either a decision tree or a neural network. Previously, we only used `age` and `gender` as predictors. Please select one other predictor variable to add to your models. 

## Loading the Data in

First, we'll load the data and downsample it. Depending on which model you choose, you may need to normalize the feature space, too.

```{r, message=FALSE}
library(tidyverse)
library(caret)
library(ROCR)
library(e1071)

## load dataset
cvd_patient <- readRDS('data/cvd_data.Rds')

## grab indices of the dataset that represent 80% of the data
train_idx <- createDataPartition(y = cvd_patient$cvd, p = 0.80, list=FALSE)

## select the rows
train_data <- cvd_patient[train_idx,]
test_data <- cvd_patient[-train_idx,]

## downsample our training dataset to the lowest-represented class and remove the 'Class' column
train_data <- downSample(train_data, train_data$cvd) %>%
  select(-Class)

shuffle_idx <- sample(nrow(train_data))
train_data <- train_data[shuffle_idx,]
```


## Build your logistic regression model below

Using the above `train_data` dataset and your new predictor, specify and build your logistic regression model (it should be of the form `cvd ~ age + gender + ...`. Use the test data to produce the predicted probabilities. Decide on a cutoff for your model based on a histogram of the predicted probabilities. Assess your model's performance by producing a confusion matrix.

```{r}
## put your code here
```


## Build your other machine learning model below

Choose one of the machine learning methods (decision trees or neural nets) and train your model that includes the new predictor using `train_data`. Assess your model performance based on the confusion matrix for your model.

```{r}
## put your code here
```


## Question 1

What was your reason for including your predictor variable? No need to cite any sources here, a simple justification will suffice.

Answer:


## Question 2
What is the AUC of your logistic regression model? Accuracy? Sensitivity? Specificity? How interpretable is your model?

Answer: 

```{r}
## put your code here
```


## Question 3

What is the AUC of your other machine learning model? Accuracy? Sensitivity? Specificity? How interpretable is your model?

Answer: 

```{r}
## put your code here
```


## Question 4

While accuracy can be a good measure of model performance, it assumes that each class is evenly represented. For diseases and disorders, this is often not the case, so we instead evaluate sensitivity (percentage of true positives called positive by the model), specificity (percentage of true negatives called negative by the model), and a combination of the two (area under the receiver operating characteristics curve; AUC).

For the three following scenarios, would you: 

1) prioritize sensitivity or specificity? Why?  
2) Would you care if the model was interpretable? Why or why not?

Start by articulating the values and priorities at each scenario. 


A) You are a 45 year-old female with an increased BMI who recently quit smoking. You know you ought to exercise some more and improve your diet, but your screen says you're negative for cardiovascular disease.

Answer: 


B) You are a healthcare provider at a large healthcare organization. You want to make sure that you don't miss any potentical CVD patients.

Answer: 


C) You are an executive at a large healthcare organization and want to target members for a cardiac disease prevention program.

Answer: 


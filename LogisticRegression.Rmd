---
title: "Logistic Regression in a Nutshell"
author: "Ted Laderas"
date: "August 18, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here are some tips on getting started on running your analysis. One of the skills you will need to bring as a data scientist is interpreting the results. 

## Separating a small portion of the data out for testing

One of the things we might like to check is the predictive power of the model. For this reason, we want to save a little bit of our data that the model doesn't "see" for testing the predicitve power of the model.

```{r}
#load analytic data
analytic <- read.delim("analytic-table-LACE.txt")

#show analytic table
analytic[1:10,]
```

## The Formula Interface for R

One of the most confusing things about R is the formula interface. The thing to remember is that formulas have a certain form. If `Y` is our dependent variable and `X1`, `X2` are independent variables, then the formula to predict `Y` has the format `Y ~ X1 + X2`. Usually these variables come from a data.frame, which is supplied by the `data` argument to the function. Note that we don't need quotes to refer to the variables in the data.frame.

## Logistic Regression

Here we load the analytic data with our LACE scores and perform a logistic regression using `Readmit30` as our dependent variable and our `LScore` and `AScore` as our independent variables. A logistic regression is a type of regression where the *outcome*, or *dependent variable* is a categorical variable (in our case, whether a patient was a readmission or not). The output is a model that predicts the log of the odds ratio being `1`, or in our case, a readmission.

```{r}
#show variable names in analytic data.frame
colnames(analytic)

library(dplyr)
analytic <- analytic %>% mutate(LACE = LScore + AScore + CScore + EScore)
laceModel <- glm(Readmit30 ~ LACE, data=analytic, family="binomial")

#run a simple logistic regression model just using LScore and Ascore
laModel <- glm(Readmit30 ~ LScore + AScore, family = "binomial", data=analytic)
```

What is the Odds Ratio? Remember that odds can be expressed as `numberCases:numberNonCases`. For example, an odds of 5:1 (win:lose) means that 5 times out of 6, we expect to win, and 1 times out of 6 we expect not to win. The odds ratio (OR) is just `numberCases/numberNonCases`. In the case of 5:1 odds, the OR is 5/1 = 5.

Remember, we can identify `oddsRatio(Readmit=1)` as `prob(Readmit=1)/prob(Readmit=0)` This is also equal to `prob(Readmit=1)/1-prob(Readmit=1)` because `prob(Readmit=0) = 1-prob(Readmit=1)` by definition. So, our logistic model is defined as:

`log(OddRatio(Readmit=1)) = log(prob(Readmit=1)/1-prob(Readmit=1)) = Constant + CoefA * LScore + CoeffA * AScore`

if we exponentiate both sides, remembering that `exp(A+B) = exp(A) * exp(B)`:

`OddsRatio(Readmit=1) = exp(CoefL * LScore + CoeffA * AScore)`

Moving things around, we get:

`OddsRatio(Readmit=1) = exp(prob(Readmit=1)/1-prob(Readmit=1)) = exp(CoefL* LScore) * exp(CoefA*AScore)`

So we find that the `OddsRatio(Readmit = 1)` is calculated by multiplying `exp(LScore)` and `exp(AScore)`. Now we can interpret the coefficients as odds.

```{r}
#Summarize the model
summary(laModel)

#grab coefficients themselves
coef(laModel)
```

In order to interpret the coefficients in terms of odds, we need to exponentiate them.

```{r}
expCoefs  <- exp(coef(laModel))
expCoefs
```

This means that for a 1 unit increase in `LScore`, the odds(Readmit score) is 1 increases by `r (1 - expCoefs["LScore""]) * 100` percent. 

The interpretation of the `AScore` is different because we're treating it as categorical data (remember, if no admission through ER = 0, and admission through ER = 3). If there is an admission through emergency room, there is a `r (1- expCoefs["AScore"] * 100` percent increase in odds(Readmit score is 1).

## Using models for prediction

Now you have built a model. What next? If you want to see the values that the model predicts for the dependent variable, you can use `predict()`. If you look at the help entry for `predict.glm` it mentions that setting the option of `type` to be `response`, you can directly get the probabilities from the model.

```{r}
modelPredictedProbabilities <- predict(laModel, type = "response")

plot(analytic$LScore, modelPredicts)
```

If you do not specify the `type` parameter, `predict()` returns the log(OddsRatio). In order to get the predicted odds ratio for the paramters, you will need to exponentiate the output using `exp()`.

Which is better? Statisticians prefer probabilities, but most people understand Odds Ratios more readily.

```{r}
modelPredictedLogOddsRatio <- predict(laModel)

modelPredictedOddsRatio <- exp(modelPredictedLogOddsRatio)
modelPredictedOddsRatio[1:10,]

#get the coefficients for the model in odds ratio scale
exp(coef(laModel))
```

##Predicting 

##Comparing Models

So we have run two models (our `lModel` and our `laModel`) and we want to compare them. Both of these models have significant terms, and it seems like the `laModel` is better. But is it 

```{r}
#run another model just using L score:
lModel <- glm(Readmit30 ~ LScore, family = "binomial", data=analytic)

summary(lModel)

#we can compare two models using anova
#this tests whether the difference between models is significant
anova(lModel, laModel, test="Chisq")
```

## More Info on Logistic Regression 

This page https://www.r-bloggers.com/evaluating-logistic-regression-models/ does a nice job explaining how to run logistic regressions and various ways to evaluate logistic regression models. 

https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/ is also a good resource for understanding logistic regression models. This page http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm is a good page for understanding odds ratios and predicted probabilities.
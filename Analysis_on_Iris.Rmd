---
title: "Analysis on Iris"
author: "Ted Laderas & Mark Klick"
date: "June 1, 2017"
output: 
  html_document:
    toc: true
    depth: 3
    number_sections: true
    theme: united
---
# Introduction

In the first section of this Markdown document, I show how to run a strawman logistic regression on the iris dataset using the `glm` package. 

In the second section, I show how to run multiple machine learning methods on the iris dataset using the `caret` package. `caret` has a unified interface to over 140 modeling and predictive techniques, so it will be ideal for us to compare our methods. There is way more information about using `caret` here: https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf

Before you do anything else, make sure that you have the following packages installed. This function will download the required packages for you if you'd like.

## Load required R Packages

```{r warning=FALSE, message=FALSE}
# ipak function: install and load multiple R packages. (stevenworthington/ipak.R)
# check to see if packages are installed. Install them if they are not, then load them into the R session.

ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

required_packages <- c('caret','dplyr','data.table','dtplyr','rpart','party','e1071','ROCR')
ipak(required_packages)
```

```{r warning=FALSE, message=FALSE}
library(caret)
library(dplyr)
library(data.table)
library(dtplyr)
library(rpart)
library(party)
library(e1071)
library(ROCR)
library(ggplot2)
library(reshape2)
```

## Some Simple Data Transformation

The first step is to reduce the Iris dataset to a 2-class problem, to make it easier to understand. This is referred to as a supervised binary classfication problem. We'll use this dataset with a variety of machine learning algorithms in part 2 of the assignment.

```{r message=FALSE}
#Here I set the random seed so I can reproduce all of the random subsetting
set.seed(11111)

#load the iris dataset
data(iris)

#select only "versicolor" and "virginica" species from data
#we need to recast Species as a factor to drop the "versicolor" level
iris2 <- iris %>% filter(Species %in% c("versicolor", "virginica")) %>% mutate(Species = factor(Species))

#confirm that we did the subsetting correct
summary(iris2)
```

# Exploratory Data Analysis (EDA) of Our Data

Before we even start to model, we want to have an idea of how our *dependent variable* `Species` is distributed among our *independent variables*. One simple way to do this is to do cross-tabs of the data. Below, we calculate a cross-table between `Sepal.Length` and `Species`. 

```{r}
#is there a correlation between Species and Sepal.Length? Let's try and visualize the results from this cross-tab
species_sepallength_Tab <- table(iris2$Species, iris2$Sepal.Length)
species_sepallength_Tab
```

We also show an example of some simple data manipulation in R.

```{r}
#make a dataframe to practice manipulating data in R
species_sepallength_Tab <- data.frame(species_sepallength_Tab)

#example of how we subset data frames in R
#here we will explicitly separate the two classes and look at the distribution of Sepal.Length
versicolor_samples <- species_sepallength_Tab[which(species_sepallength_Tab$Var1 == 'versicolor'),]
versicolor_samples$Var1<-NULL
plot(versicolor_samples)

virginica_samples <- species_sepallength_Tab[which(species_sepallength_Tab$Var1 == 'virginica'),]
virginica_samples$Var1<-NULL
plot(virginica_samples)
```

The conditional histogram we make below is a simple yet powerful EDA tool. Here we look at `Species` and `Sepal.Length`. We can visually see from this chart that there is a distinct difference in the distribution of `Sepal.Length` between our two `Species`/classes.

Question : Can you make conditional histograms looking at the other *independent* variables?

```{r}
#Here we use conditional histograms to look at any correlation between Species and Sepal.Length
ggplot(iris2,aes(x=Sepal.Length,fill=Species)) + geom_histogram(position = 'identity') + theme(text = element_text(size = 16))

#different flavor 
ggplot(iris2,aes(x=Sepal.Length,fill=Species)) + geom_histogram(position = 'dodge')+ theme(text = element_text(size = 16))
```

# Building Test and Train Datasets for our Supervised Binary Classification

Here we subset the data by building a training set to train our learner, and holding out part of the data to assess the performance on the dataset. Note that you won't have to do this on the cvd dataset, since we've already separated the two out for you. 

```{r}
#grab indices of the dataset that represent 80% of the data
trainingIndices <- createDataPartition(y = iris2$Species, p=.80,
                                       list=FALSE)

#this is alternatively how we'd subset our training and test sets without using caret's createDataPartition function
# dataSize <- nrow(iris2$Species)
# trainSize <-  floor(0.80* dataSize)
# trainIndex <-  sample(dataSize,size = trainSize,replace = FALSE)

#show the first few training indices
trainingIndices[1:10]

#select the rows
trainData <- iris2[trainingIndices,]
#confirm the number of rows (should be 80)
nrow(trainData)

#build our test set using the R-indexing
#using the "-" operator
testData <- iris2[-trainingIndices,]

#confirm the number of rows (should be 20) 
nrow(testData)
```

In summary: our training data has `r nrow(trainData)` samples and our training set has `r nrow(testData)` samples.

# Training Your Machine Learner

Here we'll train our straw man logistic regression algorithm using the 'glm' package. In part two of the assignment, we'll use multiple machine learning algorithms using the `train` function.

Before you get started, you should have selected your cohort.

Think very carefully before you add a predictive feature/variable. Using all features will actually impact the interpretability of your model. You need to justify the inclusion of each variable. Here I've included only `Sepal.Width` and `Petal.Width` because you could argue that `Sepal.Length` and `Sepal.Width` might be providing the same information.

Here I'll show how to use logistic regression using the 'glm' package ('glm' is provided with any default R installation).

## The Formula Interface for R

One of the most confusing things about R is the formula interface. The thing to remember is that formulas have a certain form. If `Y` is our dependent variable and `X1`, `X2` are independent variables, then the formula to predict `Y` has the format `Y ~ X1 + X2`. Usually these variables come from a data.frame, which is supplied by the `data` argument to the function. Note that we don't need quotes to refer to the variables in the data.frame.

# Logistic Regression

Here we perform a logistic regression using `Species` as our dependent variable and our `Sepal.Width` and `Petal.Width` as our independent variables. A logistic regression is a type of regression where the *outcome*, or *dependent variable* is related to the probability of categorical variable being true (in our case, whether a sample was a particular `Species`). The output is a model that predicts the class of a given sample, for our example, `Species`.

```{r}
#our straw man: logistic regression
logitIris <-  glm(Species ~ Sepal.Width + Petal.Width, family = "binomial", data=trainData)
summary(logitIris)
```

## Interpreting Logistic Regression Models

Let's look at the output of our model. This gives us the coefficients on the logit scale. We look at the model coeffiecients for each *independent variable*.

```{r}
coefs <-  coef(logitIris)
coefs
```

The estimates from logistic regression characterize the relationship between the predictor or *independent* and response or *dependent* variable on a log-odds scale. Because this isn’t of much practical value, we’ll usually want to use the exponential function to calculate the odds ratios for each preditor.

```{r}
expCoefs  <- exp(coefs)
expCoefs
```

This informs us that for every one unit increase in `Petal.Width`, the odds of being classified correctly increase by a huge factor! In many cases, we often want to use the model parameters to predict the value of the target variable in a completely new set of observations.

# Assessing the logistic regression model on the Test Set

Now that we have our logistic model trained, we can evaluate the model on our test dataset. To do this, we use the `predict` function, and pass both our trained learner `logitIris` and our `testData` into `predict`.

Question: why don't we assess our models on the training data? What are the dangers of doing so?

```{r}
classPredLogit <- predict(logitIris,newdata = testData,type='response')
```

## Classification Accuracy 

Evaluate our model based on how accurately it classifies test set samples as the correct Species.

```{r}
truthPredict <-  table(testData$Species, classPredLogit)

#number of cases (should be 20)
totalCases <-  sum(truthPredict)
totalCases

#number of misclassified samples
misclassified <- truthPredict[1,2] + truthPredict[2,1]
misclassified

accuracy <- (totalCases - misclassified) / totalCases
accuracy
```

## ROC 

We can summarize our model performance using the `ROCR` package (be sure to install it using `install.packages("ROCR")`). 

An ROC curve (Receiver-Operator-Characteristic) is a measure of classifier performance. Sensitivity (our ability to detect true positives) and Specificity (our ability to detect true negatives) are just as important as accuracy when we are evaluating our model. Using the ROCR package we generate a graphic that shows the trade off between the rate at which you can correctly predict something with the rate of incorrectly predicting something

```{r}
# Compute AUC for predicting Class with the model
prob <- predict(logitIris, newdata=testData, type="response")
pred <- prediction(prob, testData$Species)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
```

The area under the ROC curve (AUC) is one way we can summarize our model performance. A model with perfect predictive ability has an AUC of 1. A random test (that is, a coin flip) has an AUC of 0.5. 

```{r}
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

## More Info on Logistic Regression 

Please note: this is a brief, non-comprehensive introduction to utilizing logistic regression for this class example. In addition to the R links at the end of this section, we highly recommend texts such as *Applied Logistic Regression* (Hosmer, Lemeshow and Sturidvant) for more detailed treatment of logistic regression, including topics such as model building strategies, diagnostics and assessment of fit as well as more complex designs which are beyond the scope of this assignment.

This page https://www.r-bloggers.com/evaluating-logistic-regression-models/ does a nice job explaining how to run logistic regressions and various ways to evaluate logistic regression models. 

https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/ is also a good resource for understanding logistic regression models. This page http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm is a good page for understanding odds ratios and predicted probabilities.


# Part Two: Running multiple machine learning methods on the Iris dataset

You can use a variety of other machine learning methods conveniently wrapped in the 'caret' package! I'll show how to use two other modeling methods here other than our straw man logistic regression.  

  + `lda` - linear discriminant analysis, 
  + `rpart` - Classification and regression trees 

A full list of machine learning methods in `caret` is available here: http://topepo.github.io/caret/available-models.html Note that you will also have to install the corresponding packages listed for that method.

```{r warning=FALSE, message=FALSE}
#our straw man: logistic regression
logitIris <- train(Species ~ Sepal.Width + Petal.Width, method="glm", family="binomial",
                   data=trainData)

#train linear discriminant analysis method
ldaIris <- train(Species ~ Sepal.Width + Petal.Width, method= "lda", data=trainData)

#train classification and regression tree
cartIris <- train(Species ~ Sepal.Width + Petal.Width, method= "rpart", data=trainData)
```

## Assessing the models on the Test Set

Now that we have our models trained, we can evaluate them on our test dataset. To do this, we use the `predict` function, and pass both our trained learner `ldaIris` and our `testData` into `predict`.

```{r}
#Predict species on test data
classPredLDA <- predict(ldaIris, newdata=testData)

#Compare predictions directly with the truth
data.frame(classPredLDA, truth=testData$Species)
```

Evaluate our LDA model based on how accurately it classifies test set samples as the correct Species.

```{r}
truthPredict_lda <-  table(testData$Species, classPredLDA)

#number of cases (should be 20)
totalCases_lda <-  sum(truthPredict)
totalCases_lda

#number of misclassified samples
misclassified_lda <- truthPredict_lda[1,2] + truthPredict_lda[2,1]
misclassified_lda

accuracy_lda <- (totalCases_lda - misclassified_lda) / totalCases_lda
accuracy_lda
```

We can also use the `caret` package to make the confusion matrices more quickly! Luckily when we compare the accuracy measures compute by our method and `caret` they are the same.

```{r}
#calculate confusion Matrix and other measures of accuracy
confMatLDA <- confusionMatrix(testData$Species, classPredLDA)

#Show everything from `confusionMatrix`
confMatLDA

#access confusion matrix directly
confMatLDA$table

#Show accuracy values
confMatLDA$overall

#Show class agreement values
confMatLDA$byClass
```

## So which algorithm did best?

Let's run our predictions on the other learners as well, and compare accuracies:

```{r}
classPredCart <- predict(cartIris, newdata = testData)
classPredLogit <- predict(logitIris, newdata = testData)

#compare all the predictions directly
#were there any rows where the predictions didn't match?
data.frame(truth=testData$Species,logit=classPredLogit, LDA=classPredLDA, CART=classPredCart)
```

### Comparing Accuracies of our models

Here we compare the accuracies of our models.

```{r}
confMatCart <- confusionMatrix(classPredCart, testData$Species)
confMatLogit <- confusionMatrix(classPredLogit, testData$Species)

accuracyComparison = rbind(logit = confMatLogit$overall,
                       LDA = confMatLDA$overall,
                       CART = confMatCart$overall
                    )

accuracyComparison
```